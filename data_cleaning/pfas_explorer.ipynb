{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\zuhay_xe4gjlw\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Collecting geopandas\n",
      "  Downloading geopandas-1.0.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\zuhay_xe4gjlw\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zuhay_xe4gjlw\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zuhay_xe4gjlw\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zuhay_xe4gjlw\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Collecting pyogrio>=0.7.2 (from geopandas)\n",
      "  Downloading pyogrio-0.10.0-cp312-cp312-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\zuhay_xe4gjlw\\anaconda3\\lib\\site-packages (from geopandas) (24.1)\n",
      "Collecting pyproj>=3.3.0 (from geopandas)\n",
      "  Downloading pyproj-3.7.1-cp312-cp312-win_amd64.whl.metadata (31 kB)\n",
      "Collecting shapely>=2.0.0 (from geopandas)\n",
      "  Downloading shapely-2.1.0-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\zuhay_xe4gjlw\\anaconda3\\lib\\site-packages (from pyogrio>=0.7.2->geopandas) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zuhay_xe4gjlw\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading geopandas-1.0.1-py3-none-any.whl (323 kB)\n",
      "Downloading pyogrio-0.10.0-cp312-cp312-win_amd64.whl (16.2 MB)\n",
      "   ---------------------------------------- 0.0/16.2 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 7.6/16.2 MB 36.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.0/16.2 MB 40.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.2/16.2 MB 31.8 MB/s eta 0:00:00\n",
      "Downloading pyproj-3.7.1-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.3/6.3 MB 38.5 MB/s eta 0:00:00\n",
      "Downloading shapely-2.1.0-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 23.3 MB/s eta 0:00:00\n",
      "Installing collected packages: shapely, pyproj, pyogrio, geopandas\n",
      "Successfully installed geopandas-1.0.1 pyogrio-0.10.0 pyproj-3.7.1 shapely-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/raw/pfas.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/raw/pfas.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,  encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/raw/pfas.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/raw/pfas.csv',  encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_gm_result = df['gm_samp_collection_date'].min()\n",
    "max_gm_result = df['gm_samp_collection_date'].max()\n",
    "min_gm_result, max_gm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'gm_samp_collection_date' to datetime if not already done\n",
    "df['gm_samp_collection_date'] = pd.to_datetime(df['gm_samp_collection_date'])\n",
    "\n",
    "# Filter the dataset for dates between 2017 and 2021\n",
    "filtered_df = df[(df['gm_samp_collection_date'] >= '2017-01-01') & (df['gm_samp_collection_date'] <= '2021-12-31')]\n",
    "\n",
    "df = filtered_df\n",
    "\n",
    "# Display the filtered dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gm_chemical_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gm_chemical_vvl'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gm_well_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isna().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop values where gm_result is NaN\n",
    "df = df.dropna(subset=['gm_result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'gm_chemical_name' and 'year', then calculate the mean of 'gm_result'\n",
    "average_result = df.groupby(['gm_chemical_name'])['gm_result'].mean().reset_index()\n",
    "\n",
    "# Display the result\n",
    "average_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_counties = ['Alameda', 'Alpine', 'Amador', 'Butte', 'Calaveras', 'Colusa',\n",
    "\t'Contra Costa', 'Del Norte', 'El Dorado', 'Tulare', 'Fresno',\n",
    "\t'Glenn', 'Humboldt', 'Imperial', 'Inyo', 'Kern', 'Kings', 'Lake',\n",
    "\t'Lassen', 'Los Angeles', 'Madera', 'Marin', 'Mariposa',\n",
    "\t'Mendocino', 'Merced', 'Modoc', 'Mono', 'Monterey', 'Napa',\n",
    "\t'Nevada', 'Orange', 'Placer', 'Plumas', 'Riverside', 'Sacramento',\n",
    "\t'San Benito', 'San Bernardino', 'San Diego', 'San Francisco',\n",
    "\t'San Joaquin', 'San Luis Obispo', 'San Mateo', 'Santa Barbara',\n",
    "\t'Santa Clara', 'Santa Cruz', 'Shasta', 'Sierra', 'Siskiyou',\n",
    "\t'Solano', 'Sonoma', 'Stanislaus', 'Sutter', 'Shasta', 'Tehama',\n",
    "\t'Trinity', 'Tulare', 'Tuolumne', 'Ventura', 'Kern', 'Yolo', 'Yuba'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "df = df[['gm_chemical_vvl', 'gm_result', 'gm_samp_collection_date', 'gm_latitude', 'gm_longitude']]\n",
    "df = df.sort_values(by='gm_samp_collection_date', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load the shapefile\n",
    "shapefile_path = '../data/raw/tl_2022_us_county.zip'\n",
    "gdf = gpd.read_file(shapefile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the bounds for each county\n",
    "county_bounds = {}\n",
    "for _, row in gdf.iterrows():\n",
    "\tcounty_name = row['NAME']  # Adjust this to match the column name in your shapefile\n",
    "\tbounds = row['geometry'].bounds\n",
    "\tcounty_bounds[county_name] = {\n",
    "\t\t'min_lat': bounds[1],\n",
    "\t\t'max_lat': bounds[3],\n",
    "\t\t'min_lon': bounds[0],\n",
    "\t\t'max_lon': bounds[2]\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_county_bounds = {name: county_bounds[name] for name in california_counties}\n",
    "california_county_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_county_from_bounds(lat, lon):\n",
    "\tfor county, bounds in california_county_bounds.items():\n",
    "\t\tif bounds['min_lat'] <= lat <= bounds['max_lat'] and bounds['min_lon'] <= lon <= bounds['max_lon']:\n",
    "\t\t\treturn county\n",
    "\treturn 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['county'] = df.apply(lambda row: get_county_from_bounds(row['gm_latitude'], row['gm_longitude']), axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['county'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['gm_chemical_vvl', 'gm_result', 'gm_samp_collection_date', 'county']]\n",
    "df = df.sort_values(by='gm_samp_collection_date', ascending=False)\n",
    "df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby(['county', 'gm_samp_collection_date', 'gm_chemical_vvl'])['gm_result'].mean().reset_index()\n",
    "grouped_df = grouped_df.sort_values(by='gm_samp_collection_date', ascending=False)\n",
    "# Display the grouped dataframe\n",
    "grouped_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame to wide format\n",
    "wide_df = grouped_df.pivot(index=['county', 'gm_samp_collection_date'], columns='gm_chemical_vvl', values='gm_result').reset_index()\n",
    "\n",
    "# Display the wide format DataFrame\n",
    "wide_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the columns and rename them\n",
    "wide_df.columns.name = None\n",
    "wide_df.columns = ['county', 'gm_samp_collection_date'] + [f'{col}' for col in wide_df.columns[2:]]\n",
    "\n",
    "wide_df = wide_df.sort_values(by=['county', 'gm_samp_collection_date'], ascending=False)\n",
    "\n",
    "# Display the wide format DataFrame with renamed columns\n",
    "wide_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'gm_samp_collection_date' column is in datetime format\n",
    "wide_df['gm_samp_collection_date'] = pd.to_datetime(wide_df['gm_samp_collection_date'])\n",
    "\n",
    "# Extract the year from the 'gm_samp_collection_date' column\n",
    "wide_df['year'] = wide_df['gm_samp_collection_date'].dt.year\n",
    "\n",
    "# Calculate the annual mean for each PFAS by county and year\n",
    "annual_means = wide_df.groupby(['county', 'year']).mean().reset_index()\n",
    "annual_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fill missing values with the annual mean\n",
    "def fill_with_annual_mean(row, annual_means):\n",
    "\tcounty = row['county']\n",
    "\tyear = row['year']\n",
    "\tfor col in annual_means.columns[2:]:  # Skip 'county' and 'year' columns\n",
    "\t\tif pd.isna(row[col]):\n",
    "\t\t\tmean_value = annual_means[(annual_means['county'] == county) & (annual_means['year'] == year)][col]\n",
    "\t\t\tif not mean_value.empty:\n",
    "\t\t\t\t\trow[col] = mean_value.values[0]\n",
    "\treturn row\n",
    "\n",
    "# Apply the function to fill missing values\n",
    "wide_df_filled = wide_df.apply(lambda row: fill_with_annual_mean(row, annual_means), axis=1)\n",
    "\n",
    "# Drop the 'year' column as it's no longer needed\n",
    "wide_df_filled = wide_df_filled.drop(columns=['year'])\n",
    "\n",
    "wide_df_filled = wide_df_filled.sort_values(by=['county', 'gm_samp_collection_date'], ascending=False)\n",
    "\n",
    "# Display the DataFrame with filled values\n",
    "wide_df_filled.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with the mean for all columns except 'county' and 'gm_samp_collection_date'\n",
    "columns_to_fill = wide_df_filled.columns.difference(['county', 'gm_samp_collection_date'])\n",
    "wide_df_filled[columns_to_fill] = wide_df_filled[columns_to_fill].fillna(wide_df_filled[columns_to_fill].mean())\n",
    "\n",
    "# Display the DataFrame with filled values\n",
    "wide_df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfas_missing_values_by_county = wide_df_filled.isna().sum()\n",
    "pfas_missing_values_by_county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_filled = wide_df_filled.fillna(0)\n",
    "wide_df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_filled = wide_df_filled.sort_values(by=['county', 'gm_samp_collection_date'])\n",
    "wide_df_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Apply log transformation to all PFAS columns (excluding county and date)\n",
    "pfas_columns = wide_df_filled.columns.difference(['county', 'gm_samp_collection_date'])\n",
    "\n",
    "# Add small constant (1) before taking log to handle zeros\n",
    "wide_df_filled[pfas_columns] = np.log1p(wide_df_filled[pfas_columns])\n",
    "\n",
    "# Display first few rows of log-transformed data\n",
    "wide_df_filled.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of PFAS chemical columns (excluding county and date)\n",
    "pfas_columns = wide_df_filled.columns.difference(['county', 'gm_samp_collection_date'])\n",
    "\n",
    "# Calculate total PFAS concentration for each row by summing across all PFAS columns\n",
    "wide_df_filled['total_pfas_concentration'] = wide_df_filled[pfas_columns].sum(axis=1)\n",
    "\n",
    "# Display the first few rows to verify the total_pfas_concentration column\n",
    "print(\"First few rows with total PFAS concentrations:\")\n",
    "wide_df_filled[['county', 'gm_samp_collection_date', 'total_pfas_concentration']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "wide_df_filled.to_csv('../data/cleaned/cleaned_pfas.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the cleaned cancer data\n",
    "cancer_df = pd.read_csv('../data/cleaned/cleaned_cancer_dataset.csv')\n",
    "\n",
    "# Merge PFAS and cancer data on county\n",
    "merged_df = pd.merge(wide_df_filled, cancer_df, on='county', how='inner')\n",
    "\n",
    "# Display first few rows of merged dataset\n",
    "print(\"First few rows of merged PFAS and cancer data:\")\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged DataFrame to a CSV file\n",
    "merged_df.to_csv('../data/cleaned/cleaned_pfas_cancer_merged.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
